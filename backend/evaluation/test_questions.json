[
  {
    "id": 1,
    "question": "What is the main innovation introduced in the Transformer architecture and how does it differ from previous sequence-to-sequence models?",
    "category": "Architecture",
    "expected_sources": ["attention_is_all_you_need"],
    "difficulty": "medium"
  },
  {
    "id": 2,
    "question": "How does BERT's bidirectional training differ from traditional language models, and what advantages does this provide?",
    "category": "Training Method",
    "expected_sources": ["bert_pretraining"],
    "difficulty": "medium"
  },
  {
    "id": 3,
    "question": "What is few-shot learning in the context of GPT-3, and how does it demonstrate emergent capabilities?",
    "category": "Capabilities",
    "expected_sources": ["gpt3_language_models"],
    "difficulty": "hard"
  },
  {
    "id": 4,
    "question": "What modifications did RoBERTa make to BERT's training procedure and why were these changes beneficial?",
    "category": "Optimization",
    "expected_sources": ["roberta_optimization"],
    "difficulty": "medium"
  },
  {
    "id": 5,
    "question": "How does T5's text-to-text framework unify different NLP tasks, and what are some examples of this unification?",
    "category": "Framework",
    "expected_sources": ["t5_text_to_text"],
    "difficulty": "medium"
  },
  {
    "id": 6,
    "question": "Compare the attention mechanisms used in Transformer and BERT. What are the key similarities and differences?",
    "category": "Comparison",
    "expected_sources": ["attention_is_all_you_need", "bert_pretraining"],
    "difficulty": "hard"
  },
  {
    "id": 7,
    "question": "What role does scale play in the performance of large language models like GPT-3, and what evidence supports this?",
    "category": "Scaling",
    "expected_sources": ["gpt3_language_models"],
    "difficulty": "medium"
  },
  {
    "id": 8,
    "question": "How do the preprocessing steps in RoBERTa and T5 differ, and what impact do these differences have on model performance?",
    "category": "Preprocessing",
    "expected_sources": ["roberta_optimization", "t5_text_to_text"],
    "difficulty": "hard"
  },
  {
    "id": 9,
    "question": "What is the relationship between model size, training data, and computational requirements across these transformer-based models?",
    "category": "Resources",
    "expected_sources": ["attention_is_all_you_need", "bert_pretraining", "gpt3_language_models", "roberta_optimization", "t5_text_to_text"],
    "difficulty": "hard"
  },
  {
    "id": 10,
    "question": "How do these different transformer variants handle the trade-off between computational efficiency and model performance?",
    "category": "Efficiency",
    "expected_sources": ["attention_is_all_you_need", "bert_pretraining", "gpt3_language_models", "roberta_optimization", "t5_text_to_text"],
    "difficulty": "hard"
  }
] 