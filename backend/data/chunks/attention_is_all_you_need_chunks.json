[
  {
    "text": "Providedproperattributionisprovided,Googleherebygrantspermissionto reproducethetablesandfiguresinthispapersolelyforuseinjournalisticor scholarlyworks.",
    "size": 150,
    "chunk_id": 0,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "spapersolelyforuseinjournalisticor scholarlyworks. Attention Is All You Need 3202 AshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗ GoogleBrain GoogleBrain GoogleResearch GoogleResearch guA avaswani@google.com noam@google.com nikip@google.com usz@google.com LlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗ 2 GoogleResearch UniversityofToronto GoogleBrain llion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com ]LC.sc[ IlliaPolosukhin∗ ‡ illia.polosukhin@gmail.com Abstract 7v26730.6071:viXra Thedominantsequencetransductionmodelsarebasedoncomplexrecurrentor convolutionalneuralnetworksthatincludeanencoderandadecoder.",
    "size": 624,
    "chunk_id": 1,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "onalneuralnetworksthatincludeanencoderandadecoder. Thebest performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, basedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions entirely. Experiments on two machine translation tasks show these models to besuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly less time to train.",
    "size": 448,
    "chunk_id": 2,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "zableandrequiringsignificantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask, ourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after trainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe bestmodelsfromtheliterature.",
    "size": 421,
    "chunk_id": 3,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "thetrainingcostsofthe bestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto othertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith largeandlimitedtrainingdata.",
    "size": 192,
    "chunk_id": 4,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "tuencyparsingbothwith largeandlimitedtrainingdata. ∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted theefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand hasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head attentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery detail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand tensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and efficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand implementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating ourresearch.",
    "size": 843,
    "chunk_id": 5,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "rovingresultsandmassivelyaccelerating ourresearch. †WorkperformedwhileatGoogleBrain. ‡WorkperformedwhileatGoogleResearch. 31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA. 1 Introduction Recurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks inparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand transductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5].",
    "size": 453,
    "chunk_id": 6,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "chaslanguagemodelingandmachinetranslation[35,2,5]. Numerous effortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder architectures[38,24,15]. Recurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput sequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden statesh ,asafunctionoftheprevioushiddenstateh andtheinputforpositiont.",
    "size": 411,
    "chunk_id": 7,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "oftheprevioushiddenstateh andtheinputforpositiont. Thisinherently t t−1 sequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger sequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved significantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional computation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental constraintofsequentialcomputation,however,remains.",
    "size": 475,
    "chunk_id": 8,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "constraintofsequentialcomputation,however,remains. Attentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc- tionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein theinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms areusedinconjunctionwitharecurrentnetwork.",
    "size": 349,
    "chunk_id": 9,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "hanisms areusedinconjunctionwitharecurrentnetwork. InthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead relyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput. TheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin translationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.",
    "size": 377,
    "chunk_id": 10,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "ingtrainedforaslittleastwelvehoursoneightP100GPUs. 2 Background ThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU [16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding block,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels, thenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows inthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet.",
    "size": 489,
    "chunk_id": 11,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "ns,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as describedinsection3.2.",
    "size": 362,
    "chunk_id": 12,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "ith Multi-Head Attention as describedinsection3.2. Self-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions ofasinglesequenceinordertocomputearepresentationofthesequence. Self-attentionhasbeen usedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization, textualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].",
    "size": 398,
    "chunk_id": 13,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "sk-independentsentencerepresentations[4,27,28,22]. End-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence- alignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand languagemodelingtasks[34]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence- alignedRNNsorconvolution.",
    "size": 451,
    "chunk_id": 14,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "putwithoutusingsequence- alignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate self-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9]. 3 ModelArchitecture Mostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35]. Here, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence 1 n of continuous representations z = (z ,...,z ).",
    "size": 428,
    "chunk_id": 15,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "1 n of continuous representations z = (z ,...,z ). Given z, the decoder then generates an output 1 n sequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive 1 m [10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext. 2 Figure1: TheTransformer-modelarchitecture. TheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully connectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1, respectively.",
    "size": 498,
    "chunk_id": 16,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "ownintheleftandrighthalvesofFigure1, respectively. 3.1 EncoderandDecoderStacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position- wisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof the two sub-layers, followed by layer normalization [1].",
    "size": 394,
    "chunk_id": 17,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "o sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer itself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding layers,produceoutputsofdimensiond =512. model Decoder: ThedecoderisalsocomposedofastackofN =6identicallayers.",
    "size": 369,
    "chunk_id": 18,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "ecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo sub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head attentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections aroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.",
    "size": 420,
    "chunk_id": 19,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "positions from attending to subsequent positions. This masking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe predictionsforpositionicandependonlyontheknownoutputsatpositionslessthani. 3.2 Attention Anattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput, wherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum 3 ScaledDot-ProductAttention Multi-HeadAttention Figure 2: (left) Scaled Dot-Product Attention.",
    "size": 490,
    "chunk_id": 20,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "ion Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attentionlayersrunninginparallel. ofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe querywiththecorrespondingkey. 3.2.1 ScaledDot-ProductAttention Wecallourparticularattention\"ScaledDot-ProductAttention\"(Figure2). Theinputconsistsof queriesandkeysofdimensiond k,a√ndvaluesofdimensiond v.",
    "size": 419,
    "chunk_id": 21,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "iesandkeysofdimensiond k,a√ndvaluesofdimensiond v. Wecomputethedotproductsofthe querywithallkeys,divideeachby d ,andapplyasoftmaxfunctiontoobtaintheweightsonthe k values. Inpractice,wecomputetheattentionfunctiononasetofqueriessimultaneously,packedtogether intoamatrixQ. ThekeysandvaluesarealsopackedtogetherintomatricesK andV.",
    "size": 324,
    "chunk_id": 22,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "sandvaluesarealsopackedtogetherintomatricesK andV. Wecompute thematrixofoutputsas: QKT Attention(Q,K,V)=softmax( √ )V (1) d k Thetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot-product(multi- plicative)attention. Dot-productattentionisidenticaltoouralgorithm,exceptforthescalingfactor of √1 . Additiveattentioncomputesthecompatibilityfunctionusingafeed-forwardnetworkwith dk asinglehiddenlayer.",
    "size": 412,
    "chunk_id": 23,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "ingafeed-forwardnetworkwith dk asinglehiddenlayer. Whilethetwoaresimilarintheoreticalcomplexity,dot-productattentionis muchfasterandmorespace-efficientinpractice,sinceitcanbeimplementedusinghighlyoptimized matrixmultiplicationcode. Whileforsmallvaluesofd thetwomechanismsperformsimilarly,additiveattentionoutperforms k dotproductattentionwithoutscalingforlargervaluesofd [3].",
    "size": 374,
    "chunk_id": 24,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "ductattentionwithoutscalingforlargervaluesofd [3]. Wesuspectthatforlargevaluesof k d ,thedotproductsgrowlargeinmagnitude,pushingthesoftmaxfunctionintoregionswhereithas k extremelysmallgradients4. Tocounteractthiseffect,wescalethedotproductsby √1 .",
    "size": 246,
    "chunk_id": 25,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "ocounteractthiseffect,wescalethedotproductsby √1 . dk 3.2.2 Multi-HeadAttention Insteadofperformingasingleattentionfunctionwithd -dimensionalkeys,valuesandqueries, model wefounditbeneficialtolinearlyprojectthequeries,keysandvalueshtimeswithdifferent,learned linearprojectionstod ,d andd dimensions,respectively.",
    "size": 311,
    "chunk_id": 26,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "earprojectionstod ,d andd dimensions,respectively. Oneachoftheseprojectedversionsof k k v queries,keysandvalueswethenperformtheattentionfunctioninparallel,yieldingd -dimensional v 4Toillustratewhythedotproductsgetlarge,assumethatthecomponentsofqandkareindependentrandom variableswithmean0andvariance1.Thentheirdotproduct,q·k=()dk q k ,hasmean0andvarianced . i=1 i i k 4 output values. These are concatenated and once again projected, resulting in the final values, as depictedinFigure2.",
    "size": 484,
    "chunk_id": 27,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "sulting in the final values, as depictedinFigure2. Multi-headattentionallowsthemodeltojointlyattendtoinformationfromdifferentrepresentation subspacesatdifferentpositions. Withasingleattentionhead,averaginginhibitsthis. MultiHead(Q,K,V)=Concat(head ,...,head )WO 1 h wherehead =Attention(QWQ,KWK,VWV) i i i i WheretheprojectionsareparametermatricesWQ ∈Rdmodel×dk,WK ∈Rdmodel×dk,WV ∈Rdmodel×dv i i i andWO ∈Rhdv×dmodel. In this work we employ h = 8 parallel attention layers, or heads.",
    "size": 480,
    "chunk_id": 28,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "employ h = 8 parallel attention layers, or heads. For each of these we use d =d =d /h=64. Duetothereduceddimensionofeachhead,thetotalcomputationalcost k v model issimilartothatofsingle-headattentionwithfulldimensionality. 3.2.3 ApplicationsofAttentioninourModel TheTransformerusesmulti-headattentioninthreedifferentways: • In\"encoder-decoderattention\"layers,thequeriescomefromthepreviousdecoderlayer, andthememorykeysandvaluescomefromtheoutputoftheencoder.",
    "size": 455,
    "chunk_id": 29,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "ememorykeysandvaluescomefromtheoutputoftheencoder. Thisallowsevery positioninthedecodertoattendoverallpositionsintheinputsequence. Thismimicsthe typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38,2,9]. • Theencodercontainsself-attentionlayers. Inaself-attentionlayerallofthekeys,values andqueriescomefromthesameplace,inthiscase,theoutputofthepreviouslayerinthe encoder. Eachpositionintheencodercanattendtoallpositionsinthepreviouslayerofthe encoder.",
    "size": 482,
    "chunk_id": 30,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "tendtoallpositionsinthepreviouslayerofthe encoder. • Similarly,self-attentionlayersinthedecoderalloweachpositioninthedecodertoattendto allpositionsinthedecoderuptoandincludingthatposition. Weneedtopreventleftward informationflowinthedecodertopreservetheauto-regressiveproperty. Weimplementthis insideofscaleddot-productattentionbymaskingout(settingto−∞)allvaluesintheinput ofthesoftmaxwhichcorrespondtoillegalconnections. SeeFigure2.",
    "size": 430,
    "chunk_id": 31,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "axwhichcorrespondtoillegalconnections. SeeFigure2. 3.3 Position-wiseFeed-ForwardNetworks Inadditiontoattentionsub-layers,eachofthelayersinourencoderanddecodercontainsafully connectedfeed-forwardnetwork,whichisappliedtoeachpositionseparatelyandidentically. This consistsoftwolineartransformationswithaReLUactivationinbetween. FFN(x)=max(0,xW +b )W +b (2) 1 1 2 2 Whilethelineartransformationsarethesameacrossdifferentpositions,theyusedifferentparameters from layer to layer.",
    "size": 471,
    "chunk_id": 32,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "ns,theyusedifferentparameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is d = 512, and the inner-layer has dimensionality model d =2048. ff 3.4 EmbeddingsandSoftmax Similarlytoothersequencetransductionmodels,weuselearnedembeddingstoconverttheinput tokensandoutputtokenstovectorsofdimensiond .",
    "size": 383,
    "chunk_id": 33,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "input tokensandoutputtokenstovectorsofdimensiond . Wealsousetheusuallearnedlineartransfor- model mationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext-tokenprobabilities. In ourmodel,wesharethesameweightmatrixbetweenthetwoembeddinglayersandthepre-√softmax lineartransformation,similarto[30]. Intheembeddinglayers,wemultiplythoseweightsby d . model 5 Table1: Maximumpathlengths,per-layercomplexityandminimumnumberofsequentialoperations fordifferentlayertypes.",
    "size": 466,
    "chunk_id": 34,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "mberofsequentialoperations fordifferentlayertypes. nisthesequencelength,distherepresentationdimension,kisthekernel sizeofconvolutionsandrthesizeoftheneighborhoodinrestrictedself-attention.",
    "size": 188,
    "chunk_id": 35,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "hesizeoftheneighborhoodinrestrictedself-attention. LayerType ComplexityperLayer Sequential MaximumPathLength Operations Self-Attention O(n2·d) O(1) O(1) Recurrent O(n·d2) O(n) O(n) Convolutional O(k·n·d2) O(1) O(log (n)) k Self-Attention(restricted) O(r·n·d) O(1) O(n/r) 3.5 PositionalEncoding Sinceourmodelcontainsnorecurrenceandnoconvolution,inorderforthemodeltomakeuseofthe orderofthesequence,wemustinjectsomeinformationabouttherelativeorabsolutepositionofthe tokensinthesequence.",
    "size": 483,
    "chunk_id": 36,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "lativeorabsolutepositionofthe tokensinthesequence. Tothisend,weadd\"positionalencodings\"totheinputembeddingsatthe bottomsoftheencoderanddecoderstacks. Thepositionalencodingshavethesamedimensiond model astheembeddings,sothatthetwocanbesummed. Therearemanychoicesofpositionalencodings, learnedandfixed[9]. Inthiswork,weusesineandcosinefunctionsofdifferentfrequencies: PE =sin(pos/100002i/dmodel) (pos,2i) PE =cos(pos/100002i/dmodel) (pos,2i+1) whereposisthepositionandiisthedimension.",
    "size": 478,
    "chunk_id": 37,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "pos,2i+1) whereposisthepositionandiisthedimension. Thatis,eachdimensionofthepositionalencoding correspondstoasinusoid. Thewavelengthsformageometricprogressionfrom2πto10000·2π. We chosethisfunctionbecausewehypothesizeditwouldallowthemodeltoeasilylearntoattendby relativepositions,sinceforanyfixedoffsetk,PE canberepresentedasalinearfunctionof pos+k PE .",
    "size": 350,
    "chunk_id": 38,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": ",PE canberepresentedasalinearfunctionof pos+k PE . pos Wealsoexperimentedwithusinglearnedpositionalembeddings[9]instead,andfoundthatthetwo versionsproducednearlyidenticalresults(seeTable3row(E)).Wechosethesinusoidalversion becauseitmayallowthemodeltoextrapolatetosequencelengthslongerthantheonesencountered duringtraining.",
    "size": 322,
    "chunk_id": 39,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "engthslongerthantheonesencountered duringtraining. 4 WhySelf-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tionallayerscommonlyusedformappingonevariable-lengthsequenceofsymbolrepresentations (x ,...,x ) to another sequence of equal length (z ,...,z ), with x ,z ∈ Rd, such as a hidden 1 n 1 n i i layerinatypicalsequencetransductionencoderordecoder. Motivatingouruseofself-attentionwe considerthreedesiderata.",
    "size": 470,
    "chunk_id": 40,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "gouruseofself-attentionwe considerthreedesiderata. Oneisthetotalcomputationalcomplexityperlayer. Anotheristheamountofcomputationthatcan beparallelized,asmeasuredbytheminimumnumberofsequentialoperationsrequired. Thethirdisthepathlengthbetweenlong-rangedependenciesinthenetwork. Learninglong-range dependenciesisakeychallengeinmanysequencetransductiontasks. Onekeyfactoraffectingthe abilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto traverseinthenetwork.",
    "size": 481,
    "chunk_id": 41,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "wardandbackwardsignalshaveto traverseinthenetwork. Theshorterthesepathsbetweenanycombinationofpositionsintheinput andoutputsequences,theeasieritistolearnlong-rangedependencies[12]. Hencewealsocompare themaximumpathlengthbetweenanytwoinputandoutputpositionsinnetworkscomposedofthe differentlayertypes. AsnotedinTable1,aself-attentionlayerconnectsallpositionswithaconstantnumberofsequentially executed operations, whereas a recurrent layer requires O(n) sequential operations.",
    "size": 472,
    "chunk_id": 42,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "current layer requires O(n) sequential operations. In terms of computationalcomplexity,self-attentionlayersarefasterthanrecurrentlayerswhenthesequence 6 length n is smaller than the representation dimensionality d, which is most often the case with sentencerepresentationsusedbystate-of-the-artmodelsinmachinetranslations,suchasword-piece [38]andbyte-pair[31]representations.",
    "size": 375,
    "chunk_id": 43,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "hasword-piece [38]andbyte-pair[31]representations. Toimprovecomputationalperformancefortasksinvolving verylongsequences,self-attentioncouldberestrictedtoconsideringonlyaneighborhoodofsizerin theinputsequencecenteredaroundtherespectiveoutputposition. Thiswouldincreasethemaximum pathlengthtoO(n/r). Weplantoinvestigatethisapproachfurtherinfuturework. Asingleconvolutionallayerwithkernelwidthk <ndoesnotconnectallpairsofinputandoutput positions.",
    "size": 440,
    "chunk_id": 44,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "ndoesnotconnectallpairsofinputandoutput positions. DoingsorequiresastackofO(n/k)convolutionallayersinthecaseofcontiguouskernels, orO(log (n))inthecaseofdilatedconvolutions[18], increasingthelengthofthelongestpaths k betweenanytwopositionsinthenetwork. Convolutionallayersaregenerallymoreexpensivethan recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, toO(k·n·d+n·d2).",
    "size": 426,
    "chunk_id": 45,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "ease the complexity considerably, toO(k·n·d+n·d2). Evenwithk = n, however, thecomplexityofaseparable convolutionisequaltothecombinationofaself-attentionlayerandapoint-wisefeed-forwardlayer, theapproachwetakeinourmodel. Assidebenefit,self-attentioncouldyieldmoreinterpretablemodels.Weinspectattentiondistributions fromourmodelsandpresentanddiscussexamplesintheappendix.",
    "size": 367,
    "chunk_id": 46,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "urmodelsandpresentanddiscussexamplesintheappendix. Notonlydoindividualattention headsclearlylearntoperformdifferenttasks,manyappeartoexhibitbehaviorrelatedtothesyntactic andsemanticstructureofthesentences. 5 Training Thissectiondescribesthetrainingregimeforourmodels. 5.1 TrainingDataandBatching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentencepairs.",
    "size": 399,
    "chunk_id": 47,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "set consisting of about 4.5 million sentencepairs. Sentenceswereencodedusingbyte-pairencoding[3],whichhasasharedsource- targetvocabularyofabout37000tokens. ForEnglish-French,weusedthesignificantlylargerWMT 2014English-Frenchdatasetconsistingof36Msentencesandsplittokensintoa32000word-piece vocabulary[38].Sentencepairswerebatchedtogetherbyapproximatesequencelength.Eachtraining batchcontainedasetofsentencepairscontainingapproximately25000sourcetokensand25000 targettokens.",
    "size": 472,
    "chunk_id": 48,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "proximately25000sourcetokensand25000 targettokens. 5.2 HardwareandSchedule Wetrainedourmodelsononemachinewith8NVIDIAP100GPUs. Forourbasemodelsusing thehyperparametersdescribedthroughoutthepaper,eachtrainingsteptookabout0.4seconds. We trainedthebasemodelsforatotalof100,000stepsor12hours. Forourbigmodels,(describedonthe bottomlineoftable3),steptimewas1.0seconds. Thebigmodelsweretrainedfor300,000steps (3.5days). 5.3 Optimizer WeusedtheAdamoptimizer[20]withβ =0.9,β =0.98andϵ=10−9.",
    "size": 476,
    "chunk_id": 49,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "edtheAdamoptimizer[20]withβ =0.9,β =0.98andϵ=10−9. Wevariedthelearning 1 2 rateoverthecourseoftraining,accordingtotheformula: lrate=d−0.5 ·min(step_num−0.5,step_num·warmup_steps−1.5) (3) model Thiscorrespondstoincreasingthelearningratelinearlyforthefirstwarmup_stepstrainingsteps, anddecreasingitthereafterproportionallytotheinversesquarerootofthestepnumber. Weused warmup_steps=4000.",
    "size": 383,
    "chunk_id": 50,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "uarerootofthestepnumber. Weused warmup_steps=4000. 5.4 Regularization Weemploythreetypesofregularizationduringtraining: 7 Table2: TheTransformerachievesbetterBLEUscoresthanpreviousstate-of-the-artmodelsonthe English-to-GermanandEnglish-to-Frenchnewstest2014testsatafractionofthetrainingcost.",
    "size": 291,
    "chunk_id": 51,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "enchnewstest2014testsatafractionofthetrainingcost. BLEU TrainingCost(FLOPs) Model EN-DE EN-FR EN-DE EN-FR ByteNet[18] 23.75 Deep-Att+PosUnk[39] 39.2 1.0·1020 GNMT+RL[38] 24.6 39.92 2.3·1019 1.4·1020 ConvS2S[9] 25.16 40.46 9.6·1018 1.5·1020 MoE[32] 26.03 40.56 2.0·1019 1.2·1020 Deep-Att+PosUnkEnsemble[39] 40.4 8.0·1020 GNMT+RLEnsemble[38] 26.30 41.16 1.8·1020 1.1·1021 ConvS2SEnsemble[9] 26.36 41.29 7.7·1019 1.2·1021 Transformer(basemodel) 27.3 38.1 3.3·1018 Transformer(big) 28.4 41.8 2.3·1019 ResidualDropout Weapplydropout[33]totheoutputofeachsub-layer,beforeitisaddedtothe sub-layerinputandnormalized.",
    "size": 607,
    "chunk_id": 52,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": ",beforeitisaddedtothe sub-layerinputandnormalized. Inaddition,weapplydropouttothesumsoftheembeddingsandthe positionalencodingsinboththeencoderanddecoderstacks. Forthebasemodel,weusearateof P =0.1. drop LabelSmoothing Duringtraining,weemployedlabelsmoothingofvalueϵ = 0.1[36]. This ls hurtsperplexity,asthemodellearnstobemoreunsure,butimprovesaccuracyandBLEUscore.",
    "size": 360,
    "chunk_id": 53,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "rnstobemoreunsure,butimprovesaccuracyandBLEUscore. 6 Results 6.1 MachineTranslation OntheWMT2014English-to-Germantranslationtask,thebigtransformermodel(Transformer(big) inTable2)outperformsthebestpreviouslyreportedmodels(includingensembles)bymorethan2.0 BLEU,establishinganewstate-of-the-artBLEUscoreof28.4. Theconfigurationofthismodelis listedinthebottomlineofTable3. Trainingtook3.5dayson8P100GPUs.",
    "size": 398,
    "chunk_id": 54,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "ottomlineofTable3. Trainingtook3.5dayson8P100GPUs. Evenourbasemodel surpassesallpreviouslypublishedmodelsandensembles,atafractionofthetrainingcostofanyof thecompetitivemodels. OntheWMT2014English-to-Frenchtranslationtask,ourbigmodelachievesaBLEUscoreof41.0, outperformingallofthepreviouslypublishedsinglemodels,atlessthan1/4thetrainingcostofthe previousstate-of-the-artmodel. TheTransformer(big)modeltrainedforEnglish-to-Frenchused dropoutrateP =0.1,insteadof0.3.",
    "size": 461,
    "chunk_id": 55,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "lish-to-Frenchused dropoutrateP =0.1,insteadof0.3. drop Forthebasemodels,weusedasinglemodelobtainedbyaveragingthelast5checkpoints,which werewrittenat10-minuteintervals. Forthebigmodels,weaveragedthelast20checkpoints. We usedbeamsearchwithabeamsizeof4andlengthpenaltyα = 0.6[38]. Thesehyperparameters werechosenafterexperimentationonthedevelopmentset. Wesetthemaximumoutputlengthduring inferencetoinputlength+50,butterminateearlywhenpossible[38].",
    "size": 441,
    "chunk_id": 56,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "oinputlength+50,butterminateearlywhenpossible[38]. Table2summarizesourresultsandcomparesourtranslationqualityandtrainingcoststoothermodel architecturesfromtheliterature. Weestimatethenumberoffloatingpointoperationsusedtotraina modelbymultiplyingthetrainingtime,thenumberofGPUsused,andanestimateofthesustained single-precisionfloating-pointcapacityofeachGPU5.",
    "size": 357,
    "chunk_id": 57,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "single-precisionfloating-pointcapacityofeachGPU5. 6.2 ModelVariations ToevaluatetheimportanceofdifferentcomponentsoftheTransformer,wevariedourbasemodel indifferentways,measuringthechangeinperformanceonEnglish-to-Germantranslationonthe 5Weusedvaluesof2.8,3.7,6.0and9.5TFLOPSforK80,K40,M40andP100,respectively. 8 Table3: VariationsontheTransformerarchitecture. Unlistedvaluesareidenticaltothoseofthebase model. AllmetricsareontheEnglish-to-Germantranslationdevelopmentset,newstest2013.",
    "size": 481,
    "chunk_id": 58,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "h-to-Germantranslationdevelopmentset,newstest2013. Listed perplexitiesareper-wordpiece,accordingtoourbyte-pairencoding,andshouldnotbecomparedto per-wordperplexities.",
    "size": 165,
    "chunk_id": 59,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "ing,andshouldnotbecomparedto per-wordperplexities. train PPL BLEU params N d d h d d P ϵ model ff k v drop ls steps (dev) (dev) ×106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 1 512 512 5.29 24.9 4 128 128 5.00 25.5 (A) 16 32 32 4.91 25.8 32 16 16 5.01 25.4 16 5.16 25.1 58 (B) 32 5.01 25.4 60 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 (C) 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 0.0 5.77 24.6 0.2 4.95 25.5 (D) 0.0 4.67 25.3 0.2 5.47 25.7 (E) positionalembeddinginsteadofsinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 developmentset,newstest2013.",
    "size": 618,
    "chunk_id": 60,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": ".3 300K 4.33 26.4 213 developmentset,newstest2013. Weusedbeamsearchasdescribedintheprevioussection,butno checkpointaveraging. WepresenttheseresultsinTable3. InTable3rows(A),wevarythenumberofattentionheadsandtheattentionkeyandvaluedimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attentionis0.9BLEUworsethanthebestsetting,qualityalsodropsoffwithtoomanyheads. InTable3rows(B),weobservethatreducingtheattentionkeysized hurtsmodelquality.",
    "size": 486,
    "chunk_id": 61,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "hatreducingtheattentionkeysized hurtsmodelquality. This k suggests that determining compatibility is not easy and that a more sophisticated compatibility functionthandotproductmaybebeneficial. Wefurtherobserveinrows(C)and(D)that,asexpected, biggermodelsarebetter,anddropoutisveryhelpfulinavoidingover-fitting.Inrow(E)wereplaceour sinusoidalpositionalencodingwithlearnedpositionalembeddings[9],andobservenearlyidentical resultstothebasemodel.",
    "size": 440,
    "chunk_id": 62,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "],andobservenearlyidentical resultstothebasemodel. 6.3 EnglishConstituencyParsing ToevaluateiftheTransformercangeneralizetoothertasksweperformedexperimentsonEnglish constituencyparsing. Thistaskpresentsspecificchallenges: theoutputissubjecttostrongstructural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence modelshavenotbeenabletoattainstate-of-the-artresultsinsmall-dataregimes[37].",
    "size": 427,
    "chunk_id": 63,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "ainstate-of-the-artresultsinsmall-dataregimes[37]. Wetraineda4-layertransformerwithd =1024ontheWallStreetJournal(WSJ)portionofthe model PennTreebank[25],about40Ktrainingsentences. Wealsotraineditinasemi-supervisedsetting, usingthelargerhigh-confidenceandBerkleyParsercorporafromwithapproximately17Msentences [37]. Weusedavocabularyof16KtokensfortheWSJonlysettingandavocabularyof32Ktokens forthesemi-supervisedsetting.",
    "size": 415,
    "chunk_id": 64,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "ocabularyof32Ktokens forthesemi-supervisedsetting. Weperformedonlyasmallnumberofexperimentstoselectthedropout,bothattentionandresidual (section5.4),learningratesandbeamsizeontheSection22developmentset,allotherparameters remained unchanged from the English-to-German base translation model. During inference, we 9 Table4: TheTransformergeneralizeswelltoEnglishconstituencyparsing(ResultsareonSection23 ofWSJ) Parser Training WSJ23F1 Vinyals&Kaiserelal. (2014)[37] WSJonly,discriminative 88.3 Petrovetal.",
    "size": 500,
    "chunk_id": 65,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "(2014)[37] WSJonly,discriminative 88.3 Petrovetal. (2006)[29] WSJonly,discriminative 90.4 Zhuetal. (2013)[40] WSJonly,discriminative 90.4 Dyeretal. (2016)[8] WSJonly,discriminative 91.7 Transformer(4layers) WSJonly,discriminative 91.3 Zhuetal. (2013)[40] semi-supervised 91.3 Huang&Harper(2009)[14] semi-supervised 91.3 McCloskyetal. (2006)[26] semi-supervised 92.1 Vinyals&Kaiserelal. (2014)[37] semi-supervised 92.1 Transformer(4layers) semi-supervised 92.7 Luongetal.",
    "size": 465,
    "chunk_id": 66,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "ansformer(4layers) semi-supervised 92.7 Luongetal. (2015)[23] multi-task 93.0 Dyeretal. (2016)[8] generative 93.3 increasedthemaximumoutputlengthtoinputlength+300. Weusedabeamsizeof21andα=0.3 forbothWSJonlyandthesemi-supervisedsetting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur- prisinglywell,yieldingbetterresultsthanallpreviouslyreportedmodelswiththeexceptionofthe RecurrentNeuralNetworkGrammar[8].",
    "size": 452,
    "chunk_id": 67,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "heexceptionofthe RecurrentNeuralNetworkGrammar[8]. IncontrasttoRNNsequence-to-sequencemodels[37],theTransformeroutperformstheBerkeley- Parser[29]evenwhentrainingonlyontheWSJtrainingsetof40Ksentences. 7 Conclusion Inthiswork,wepresentedtheTransformer,thefirstsequencetransductionmodelbasedentirelyon attention,replacingtherecurrentlayersmostcommonlyusedinencoder-decoderarchitectureswith multi-headedself-attention.",
    "size": 413,
    "chunk_id": 68,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "coderarchitectureswith multi-headedself-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-Frenchtranslationtasks,weachieveanewstateoftheart. Intheformertaskourbest modeloutperformsevenallpreviouslyreportedensembles. Weareexcitedaboutthefutureofattention-basedmodelsandplantoapplythemtoothertasks.",
    "size": 451,
    "chunk_id": 69,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "tention-basedmodelsandplantoapplythemtoothertasks. We plantoextendtheTransformertoproblemsinvolvinginputandoutputmodalitiesotherthantextand toinvestigatelocal,restrictedattentionmechanismstoefficientlyhandlelargeinputsandoutputs suchasimages,audioandvideo. Makinggenerationlesssequentialisanotherresearchgoalsofours. The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.",
    "size": 426,
    "chunk_id": 70,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "e at https://github.com/ tensorflow/tensor2tensor. Acknowledgements WearegratefultoNalKalchbrennerandStephanGouwsfortheirfruitful comments,correctionsandinspiration. References [1] JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton. Layernormalization. arXivpreprint arXiv:1607.06450,2016. [2] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointly learningtoalignandtranslate. CoRR,abs/1409.0473,2014. [3] DennyBritz,AnnaGoldie,Minh-ThangLuong,andQuocV.Le.",
    "size": 469,
    "chunk_id": 71,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "DennyBritz,AnnaGoldie,Minh-ThangLuong,andQuocV.Le. Massiveexplorationofneural machinetranslationarchitectures. CoRR,abs/1703.03906,2017. [4] JianpengCheng,LiDong,andMirellaLapata. Longshort-termmemory-networksformachine reading. arXivpreprintarXiv:1601.06733,2016. 10 [5] KyunghyunCho,BartvanMerrienboer,CaglarGulcehre,FethiBougares,HolgerSchwenk, andYoshuaBengio. Learningphraserepresentationsusingrnnencoder-decoderforstatistical machinetranslation. CoRR,abs/1406.1078,2014. [6] Francois Chollet.",
    "size": 490,
    "chunk_id": 72,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "on. CoRR,abs/1406.1078,2014. [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprintarXiv:1610.02357,2016. [7] JunyoungChung,ÇaglarGülçehre,KyunghyunCho,andYoshuaBengio. Empiricalevaluation ofgatedrecurrentneuralnetworksonsequencemodeling. CoRR,abs/1412.3555,2014. [8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural networkgrammars. InProc.ofNAACL,2016. [9] JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin.",
    "size": 500,
    "chunk_id": 73,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "elAuli,DavidGrangier,DenisYarats,andYannN.Dauphin. Convolu- tionalsequencetosequencelearning. arXivpreprintarXiv:1705.03122v2,2017. [10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850,2013. [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,pages770–778,2016.",
    "size": 440,
    "chunk_id": 74,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "Vision and Pattern Recognition,pages770–778,2016. [12] SeppHochreiter,YoshuaBengio,PaoloFrasconi,andJürgenSchmidhuber. Gradientflowin recurrentnets: thedifficultyoflearninglong-termdependencies,2001. [13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780,1997. [14] ZhongqiangHuangandMaryHarper. Self-trainingPCFGgrammarswithlatentannotations acrosslanguages.",
    "size": 403,
    "chunk_id": 75,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "PCFGgrammarswithlatentannotations acrosslanguages. InProceedingsofthe2009ConferenceonEmpiricalMethodsinNatural LanguageProcessing,pages832–841.ACL,August2009. [15] RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu. Exploring thelimitsoflanguagemodeling. arXivpreprintarXiv:1602.02410,2016. [16] ŁukaszKaiserandSamyBengio. Canactivememoryreplaceattention? InAdvancesinNeural InformationProcessingSystems,(NIPS),2016. [17] ŁukaszKaiserandIlyaSutskever. NeuralGPUslearnalgorithms.",
    "size": 486,
    "chunk_id": 76,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "KaiserandIlyaSutskever. NeuralGPUslearnalgorithms. InInternationalConference onLearningRepresentations(ICLR),2016. [18] NalKalchbrenner,LasseEspeholt,KarenSimonyan,AaronvandenOord,AlexGraves,andKo- rayKavukcuoglu.Neuralmachinetranslationinlineartime.arXivpreprintarXiv:1610.10099v2, 2017. [19] YoonKim,CarlDenton,LuongHoang,andAlexanderM.Rush. Structuredattentionnetworks. InInternationalConferenceonLearningRepresentations,2017. [20] DiederikKingmaandJimmyBa. Adam: Amethodforstochasticoptimization.",
    "size": 494,
    "chunk_id": 77,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "ndJimmyBa. Adam: Amethodforstochasticoptimization. InICLR,2015. [21] OleksiiKuchaievandBorisGinsburg. FactorizationtricksforLSTMnetworks. arXivpreprint arXiv:1703.10722,2017. [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130,2017. [23] Minh-ThangLuong,QuocV.Le,IlyaSutskever,OriolVinyals,andLukaszKaiser. Multi-task sequencetosequencelearning.",
    "size": 474,
    "chunk_id": 78,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "kaszKaiser. Multi-task sequencetosequencelearning. arXivpreprintarXiv:1511.06114,2015. [24] Minh-ThangLuong,HieuPham,andChristopherDManning. Effectiveapproachestoattention- basedneuralmachinetranslation. arXivpreprintarXiv:1508.04025,2015. 11 [25] MitchellPMarcus,MaryAnnMarcinkiewicz,andBeatriceSantorini.Buildingalargeannotated corpusofenglish: Thepenntreebank. Computationallinguistics,19(2):313–330,1993. [26] DavidMcClosky,EugeneCharniak,andMarkJohnson. Effectiveself-trainingforparsing.",
    "size": 485,
    "chunk_id": 79,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": ",andMarkJohnson. Effectiveself-trainingforparsing. In ProceedingsoftheHumanLanguageTechnologyConferenceoftheNAACL,MainConference, pages152–159.ACL,June2006. [27] AnkurParikh,OscarTäckström,DipanjanDas,andJakobUszkoreit. Adecomposableattention model. InEmpiricalMethodsinNaturalLanguageProcessing,2016. [28] RomainPaulus,CaimingXiong,andRichardSocher. Adeepreinforcedmodelforabstractive summarization. arXivpreprintarXiv:1705.04304,2017. [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.",
    "size": 492,
    "chunk_id": 80,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "trov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on ComputationalLinguisticsand44thAnnualMeetingoftheACL,pages433–440.ACL,July 2006. [30] OfirPressandLiorWolf. Usingtheoutputembeddingtoimprovelanguagemodels. arXiv preprintarXiv:1608.05859,2016. [31] RicoSennrich,BarryHaddow,andAlexandraBirch. Neuralmachinetranslationofrarewords withsubwordunits. arXivpreprintarXiv:1508.07909,2015.",
    "size": 493,
    "chunk_id": 81,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "hsubwordunits. arXivpreprintarXiv:1508.07909,2015. [32] NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton, andJeffDean. Outrageouslylargeneuralnetworks: Thesparsely-gatedmixture-of-experts layer. arXivpreprintarXiv:1701.06538,2017. [33] NitishSrivastava,GeoffreyEHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdi- nov. Dropout: asimplewaytopreventneuralnetworksfromoverfitting. JournalofMachine LearningResearch,15(1):1929–1958,2014.",
    "size": 462,
    "chunk_id": 82,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "alofMachine LearningResearch,15(1):1929–1958,2014. [34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. InC.Cortes, N.D.Lawrence, D.D.Lee, M.Sugiyama, andR.Garnett, editors, AdvancesinNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates, Inc.,2015. [35] IlyaSutskever,OriolVinyals,andQuocVVLe. Sequencetosequencelearningwithneural networks. InAdvancesinNeuralInformationProcessingSystems,pages3104–3112,2014.",
    "size": 465,
    "chunk_id": 83,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "lInformationProcessingSystems,pages3104–3112,2014. [36] ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonathonShlens,andZbigniewWojna. Rethinkingtheinceptionarchitectureforcomputervision. CoRR,abs/1512.00567,2015. [37] Vinyals&Kaiser, Koo, Petrov, Sutskever, andHinton. Grammarasaforeignlanguage. In AdvancesinNeuralInformationProcessingSystems,2015. [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,etal.",
    "size": 484,
    "chunk_id": 84,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "rey,MaximKrikun,YuanCao,QinGao,KlausMacherey,etal. Google’sneuralmachine translationsystem: Bridgingthegapbetweenhumanandmachinetranslation. arXivpreprint arXiv:1609.08144,2016. [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forwardconnectionsforneuralmachinetranslation. CoRR,abs/1606.04199,2016. [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduceconstituentparsing.",
    "size": 455,
    "chunk_id": 85,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "Fast and accurate shift-reduceconstituentparsing. InProceedingsofthe51stAnnualMeetingoftheACL(Volume 1: LongPapers),pages434–443.ACL,August2013.",
    "size": 145,
    "chunk_id": 86,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "Volume 1: LongPapers),pages434–443.ACL,August2013. 12 Input-Input Layer5 AttentionVisualizations stnemnrevogstnemnrevog noitartsigernoitartsiger naciremAnaciremA sihtsiht tiripstirips tahttaht ytirojamytirojam evahevah dessapdessap wenwen swalswal ecnisecnis 90029002 gnikamgnikam ehteht gnitovgnitov ssecorpssecorp eromerom tluciffidtluciffid >SOE<>SOE< >dap<>dap< >dap<>dap< >dap<>dap< >dap<>dap< >dap<>dap< >dap<>dap< tItI sisi nini aa fofo roro ..",
    "size": 451,
    "chunk_id": 87,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "ap<>dap< >dap<>dap< tItI sisi nini aa fofo roro .. Figure 3: An example of the attention mechanism following long-distance dependencies in the encoderself-attentioninlayer5of6. Manyoftheattentionheadsattendtoadistantdependencyof theverb‘making’,completingthephrase‘making...moredifficult’. Attentionshereshownonlyfor theword‘making’. Differentcolorsrepresentdifferentheads. Bestviewedincolor.",
    "size": 388,
    "chunk_id": 88,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "tcolorsrepresentdifferentheads. Bestviewedincolor. 13 Input-Input Layer5 noitacilppanoitacilppanoitacilppanoitacilppa tcefreptcefreptcefreptcefrep dluohsdluohsdluohsdluohs gnissimgnissimgnissimgnissim noiniponoiniponoiniponoinipo >SOE<>SOE<>SOE<>SOE< >dap<>dap<>dap<>dap< revenrevenrevenreven ehTehTehTehT waLwaLwaLwaL lliwlliwlliwlliw ebebebeb tubtubtubtub stistististi ebebebeb tsujtsujtsujtsuj sihtsihtsihtsiht sisisisi tahwtahwtahwtahw ewewewew eraeraeraera nininini ymymymym ,,,, ---- ,,,, ....",
    "size": 499,
    "chunk_id": 89,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "eraeraeraera nininini ymymymym ,,,, ---- ,,,, .... Input-Input Layer5 Figure4: Twoattentionheads,alsoinlayer5of6,apparentlyinvolvedinanaphoraresolution. Top: Fullattentionsforhead5. Bottom: Isolatedattentionsfromjusttheword‘its’forattentionheads5 and6. Notethattheattentionsareverysharpforthisword.",
    "size": 295,
    "chunk_id": 90,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "nd6. Notethattheattentionsareverysharpforthisword. 14 Input-Input Layer5 noitacilppanoitacilppanoitacilppanoitacilppa tcefreptcefreptcefreptcefrep dluohsdluohsdluohsdluohs gnissimgnissimgnissimgnissim noiniponoiniponoiniponoinipo >SOE<>SOE<>SOE<>SOE< >dap<>dap<>dap<>dap< revenrevenrevenreven ehTehTehTehT waLwaLwaLwaL lliwlliwlliwlliw ebebebeb tubtubtubtub stistististi ebebebeb tsujtsujtsujtsuj sihtsihtsihtsiht sisisisi tahwtahwtahwtahw ewewewew eraeraeraera nininini ymymymym ,,,, ---- ,,,, ....",
    "size": 499,
    "chunk_id": 91,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  },
  {
    "text": "eraeraeraera nininini ymymymym ,,,, ---- ,,,, .... Input-Input Layer5 Figure5: Manyoftheattentionheadsexhibitbehaviourthatseemsrelatedtothestructureofthe sentence. Wegivetwosuchexamplesabove,fromtwodifferentheadsfromtheencoderself-attention atlayer5of6. Theheadsclearlylearnedtoperformdifferenttasks. 15",
    "size": 300,
    "chunk_id": 92,
    "document": "attention_is_all_you_need",
    "source_file": "data\\pdfs\\attention_is_all_you_need.pdf",
    "document_length": 35321
  }
]